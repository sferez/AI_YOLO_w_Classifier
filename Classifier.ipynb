{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IMPORTS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0W2BOLFzwpKD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CONFIG\n",
    "\n",
    "#### Project Config:\n",
    "\n",
    "- root_dir :\n",
    "    - train.csv\n",
    "    - kaggle.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ROOT_DIR = '/kaggle/'\n",
    "TRAIN_DIR = ROOT_DIR + 'data/train/'\n",
    "TEST_DIR = ROOT_DIR + 'data/test/'\n",
    "TRAIN_CSV = ROOT_DIR + 'train.csv'\n",
    "TRAIN_META_CSV = ROOT_DIR + 'train_meta.csv'\n",
    "TEST_CSV = ROOT_DIR + 'test.csv'\n",
    "CLASSIFIER = ROOT_DIR + 'classifier/'\n",
    "CLASSIFIER_TRAIN = CLASSIFIER + 'train/'\n",
    "CLASSIFIER_VAL = CLASSIFIER + 'val/'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## /!\\ TO TRIGGER IF RUNNING ON KAGGLE ONLY /!\\"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def is_kaggle():\n",
    "    \"\"\"\n",
    "    If the code is running on Kaggle take custom data from private repo and copy to root directory\n",
    "    /!\\ On Kaggle I have a private dataset (data-ai) containing: /!\\\n",
    "    - train.csv\n",
    "    - test.csv\n",
    "    - train_meta.csv\n",
    "    - kaggle.json\n",
    "    You can use your own data or download from Kaggle, just change the paths\n",
    "    \"\"\"\n",
    "    os.chdir('..')\n",
    "    !cp input/data-ai/kaggle.json .\n",
    "    !cp input/data-ai/test.csv .\n",
    "    !cp input/data-ai/train.csv .\n",
    "    !cp input/data-ai/train_meta.csv .\n",
    "    !pwd\n",
    "    !ls\n",
    "\n",
    "is_kaggle()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FUNCTIONS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DONWLOAD DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download_data(image_size=512):\n",
    "    \"\"\"\n",
    "    Download data from Kaggle\n",
    "    :param image_size: 256, 512 or 1024\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    !pip install -q kaggle\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    if image_size == 256:\n",
    "        !kaggle datasets download -d xhlulu/vinbigdata-chest-xray-resized-png-256x256\n",
    "        !unzip vinbigdata-chest-xray-resized-png-256x256.zip -d data\n",
    "    elif image_size == 512:\n",
    "        !kaggle datasets download -d xhlulu/vinbigdata\n",
    "        !unzip vinbigdata.zip -d data\n",
    "    elif image_size == 1024:\n",
    "        !kaggle datasets download -d xhlulu/vinbigdata-chest-xray-resized-png-1024x1024\n",
    "        !unzip vinbigdata-chest-xray-resized-png-1024x1024.zip -d data\n",
    "    else:\n",
    "        print(\"Image size not supported\")\n",
    "\n",
    "    print(f\"Number of train files: {len(glob(TRAIN_DIR + '*.png'))}\")\n",
    "    print(f\"Number of test files: {len(glob(TEST_DIR + '*.png'))}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DIRECTORIES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBv263mtzyCQ"
   },
   "outputs": [],
   "source": [
    "def create_directories():\n",
    "    \"\"\"\n",
    "    Create directories for the project\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    os.mkdir('classifier')\n",
    "    os.chdir('classifier')\n",
    "    os.mkdir('train')\n",
    "    os.mkdir('val')\n",
    "    os.chdir('train')\n",
    "    os.mkdir('finding')\n",
    "    os.mkdir('no_finding')\n",
    "    os.chdir('../val')\n",
    "    os.mkdir('finding')\n",
    "    os.mkdir('no_finding')\n",
    "    os.chdir('..')\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DATA PREPARATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_image_path(df_train):\n",
    "    \"\"\"\n",
    "    Add image path to train dataframe\n",
    "    :param df_train: Dataframe with train data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    df_train['image_path'] = TRAIN_DIR + df_train.image_id + '.png'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_fold_split(df_train, n_splits=6):\n",
    "    \"\"\"\n",
    "    Split train data into train and validation sets\n",
    "    :param df_train: Dataframe with train data\n",
    "    :param n_splits: Number of splits\n",
    "    :return: train_files, val_files\n",
    "    \"\"\"\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    df_train['fold'] = -1\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(df_train, groups=df_train.image_id.tolist())):\n",
    "        df_train.loc[val_idx, 'fold'] = fold\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    val_files += list(df_train[df_train.fold == fold].image_path.unique())\n",
    "    train_files += list(df_train[df_train.fold != fold].image_path.unique())\n",
    "    print(f\"Train: {len(train_files)} - Val: {len(val_files)}\")\n",
    "    return train_files, val_files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOuzndbB13sj",
    "outputId": "095eb2d3-84a0-4a25-ad38-426ef45277ac"
   },
   "outputs": [],
   "source": [
    "def copy_files(df_train):\n",
    "    \"\"\"\n",
    "    Copy files to train and validation directories\n",
    "    :param df_train: Dataframe with train data\n",
    "    :param classifier_train: Path to train directory\n",
    "    :param classifier_val: Path to validation directory\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for i, row in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "        if row.fold == 1:\n",
    "            if row['class_id'] == 14:\n",
    "                shutil.copyfile(row.image_path, CLASSIFIER_VAL + 'no_finding/' + row.image_id + '.png')\n",
    "            else:\n",
    "                shutil.copyfile(row.image_path, CLASSIFIER_VAL + 'finding/' + row.image_id + '.png')\n",
    "        else:\n",
    "            if row['class_id'] == 14:\n",
    "                shutil.copyfile(row.image_path, CLASSIFIER_TRAIN + 'no_finding/' + row.image_id + '.png')\n",
    "            else:\n",
    "                shutil.copyfile(row.image_path, CLASSIFIER_TRAIN + 'finding/' + row.image_id + '.png')\n",
    "    print(f\"Train No Finding: {len([f for f in os.listdir(CLASSIFIER_TRAIN + 'no_finding/')])}\")\n",
    "    print(f\"Train Finding: {len([f for f in os.listdir(CLASSIFIER_TRAIN + 'finding/')])}\")\n",
    "    print(f\"Val No Finding: {len([f for f in os.listdir(CLASSIFIER_VAL + 'no_finding/')])}\")\n",
    "    print(f\"Val Finding: {len([f for f in os.listdir(CLASSIFIER_VAL + 'finding/')])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiMfU8lO17xi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "06321da0-6d37-4335-bf7f-5cfe46583663"
   },
   "outputs": [],
   "source": [
    "def data_preparation():\n",
    "    \"\"\"\n",
    "    Prepare data for training\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    create_directories()\n",
    "    df_train = pd.read_csv(TRAIN_CSV)\n",
    "    df_meta = pd.read_csv(TRAIN_META_CSV)\n",
    "    add_image_path(df_train)\n",
    "    train_files, val_files = get_fold_split(df_train)\n",
    "    print('Copy files...')\n",
    "    copy_files(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DATA AUGMENTATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_transformers():\n",
    "    \"\"\"\n",
    "    Get transformers for train and validation sets\n",
    "    :return: train_transforms, val_transforms, test_transforms\n",
    "    \"\"\"\n",
    "    train_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        # torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "        # torchvision.transforms.RandomRotation(10),\n",
    "        # torchvision.transforms.RandomApply([torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)], p=0.5),\n",
    "        # torchvision.transforms.RandomApply([torchvision.transforms.GaussianBlur(3, sigma=(0.1, 2.0))], p=0.5),\n",
    "        # torchvision.transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        torchvision.transforms.RandomAffine(degrees=10, translate=(0, 0.1), scale=(1, 1.10)),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    test_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return train_transforms, val_transforms, test_transforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QogSKcz-2nBH"
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size=16):\n",
    "    \"\"\"\n",
    "    Get dataloaders for train and validation sets\n",
    "    :return: train_dataloader, val_dataloader\n",
    "    \"\"\"\n",
    "    train_transforms, val_transforms, _ = get_transformers()\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=CLASSIFIER_TRAIN, transform=train_transforms)\n",
    "    val_dataset = datasets.ImageFolder(root=CLASSIFIER_VAL, transform=val_transforms)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2,\n",
    "                                                   pin_memory=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2,\n",
    "                                                 pin_memory=True)\n",
    "    print(f\"Train: {len(train_dataset)} - Val: {len(val_dataset)}\")\n",
    "    print('Idx to class:', train_dataset.class_to_idx)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MODEL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model(model_='EfficientNet_B7'):\n",
    "    \"\"\"\n",
    "    Download model and replace last layer\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    if model_=='EfficientNet_B7':\n",
    "        model = torchvision.models.efficientnet_b7(weights=torchvision.models.EfficientNet_B7_Weights)\n",
    "        last_layer = list(model.children())[-1] # Get the last layer of the model\n",
    "        last_layer_layer = list(last_layer.children())[-1] # Get the sub-layer of the last layer\n",
    "        num_features = last_layer_layer.in_features # Get the number of input features for the sub-layer\n",
    "        last_layer[-1] = torch.nn.Linear(num_features, 1) # Replace the sub-layer with a linear layer with 1 output\n",
    "        # print(model) # Print the updated model architecture\n",
    "    elif model_=='EfficientNet_B6':\n",
    "        model = torchvision.models.efficientnet_b6(weights=torchvision.models.EfficientNet_B6_Weights)\n",
    "        last_layer = list(model.children())[-1] # Get the last layer of the model\n",
    "        last_layer_layer = list(last_layer.children())[-1] # Get the sub-layer of the last layer\n",
    "        num_features = last_layer_layer.in_features # Get the number of input features for the sub-layer\n",
    "        last_layer[-1] = torch.nn.Linear(num_features, 1) # Replace the sub-layer with a linear layer with 1 output\n",
    "        # print(model) # Print the updated model architecture\n",
    "    elif model_=='EfficientNet_B5':\n",
    "        model = torchvision.models.efficientnet_b5(weights=torchvision.models.EfficientNet_B5_Weights)\n",
    "        last_layer = list(model.children())[-1] # Get the last layer of the model\n",
    "        last_layer_layer = list(last_layer.children())[-1] # Get the sub-layer of the last layer\n",
    "        num_features = last_layer_layer.in_features # Get the number of input features for the sub-layer\n",
    "        last_layer[-1] = torch.nn.Linear(num_features, 1) # Replace the sub-layer with a linear layer with 1 output\n",
    "        # print(model) # Print the updated model architecture\n",
    "    elif model_=='EfficientNet_B4':\n",
    "        model = torchvision.models.efficientnet_b4(weights=torchvision.models.EfficientNet_B4_Weights)\n",
    "        last_layer = list(model.children())[-1] # Get the last layer of the model\n",
    "        last_layer_layer = list(last_layer.children())[-1] # Get the sub-layer of the last layer\n",
    "        num_features = last_layer_layer.in_features # Get the number of input features for the sub-layer\n",
    "        last_layer[-1] = torch.nn.Linear(num_features, 1) # Replace the sub-layer with a linear layer with 1 output\n",
    "        # print(model) # Print the updated model architecture\n",
    "    elif model_=='EfficientNet_B3':\n",
    "        model = torchvision.models.efficientnet_b3(weights=torchvision.models.EfficientNet_B3_Weights)\n",
    "        last_layer = list(model.children())[-1] # Get the last layer of the model\n",
    "        last_layer_layer = list(last_layer.children())[-1] # Get the sub-layer of the last layer\n",
    "        num_features = last_layer_layer.in_features\n",
    "        last_layer[-1] = torch.nn.Linear(num_features, 1)\n",
    "    elif model_=='resnet18':\n",
    "        model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(num_ftrs, 1)\n",
    "    elif model_=='resnet50':\n",
    "        model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(num_ftrs, 1)\n",
    "    elif model_=='resnet101':\n",
    "        model = torchvision.models.resnet101(weights=torchvision.models.ResNet101_Weights)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(num_ftrs, 1)\n",
    "    elif model_=='resnet152':\n",
    "        model = torchvision.models.resnet152(weights=torchvision.models.ResNet152_Weights)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(num_ftrs, 1)\n",
    "    elif model_=='densenet121':\n",
    "        model = torchvision.models.densenet121(weights=torchvision.models.DenseNet121_Weights)\n",
    "        num_ftrs = model.classifier.in_features\n",
    "        model.classifier = torch.nn.Linear(num_ftrs, 1)\n",
    "    elif model_=='densenet169':\n",
    "        model = torchvision.models.densenet169(weights=torchvision.models.DenseNet169_Weights)\n",
    "        num_ftrs = model.classifier.in_features\n",
    "        model.classifier = torch.nn.Linear(num_ftrs, 1)\n",
    "    elif model_=='densenet201':\n",
    "        model = torchvision.models.densenet201(weights=torchvision.models.DenseNet201_Weights)\n",
    "        num_ftrs = model.classifier.in_features\n",
    "        model.classifier = torch.nn.Linear(num_ftrs, 1)\n",
    "    else:\n",
    "        raise ValueError('Model not found, please choose one of the following: EfficientNet_B7, EfficientNet_B6, EfficientNet_B5, EfficientNet_B4, EfficientNet_B3, resnet18, resnet50, resnet101, resnet152, densenet121, densenet169, densenet201')\n",
    "\n",
    "    if torch.cuda.device_count() > 1: # check if multiple GPUs are available\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = torch.nn.DataParallel(model) # make parallel\n",
    "    model = model.to(device)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc4wjRDA2qaO"
   },
   "outputs": [],
   "source": [
    "def make_train_step(model, optimizer, loss_fn):\n",
    "    \"\"\"\n",
    "    Builds function that performs a step in the train loop\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    :param loss_fn: loss function\n",
    "    :return: train_step\n",
    "    \"\"\"\n",
    "    def train_step(x, y):\n",
    "        \"\"\"\n",
    "        Performs a train step\n",
    "        :param x: input\n",
    "        :param y: target\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        yhat = model(x)  # Make prediction\n",
    "        model.train()  #enter train mode\n",
    "        loss = loss_fn(yhat, y)  #compute loss\n",
    "        loss.backward()  # backpropagation\n",
    "        optimizer.step()  #update weights\n",
    "        optimizer.zero_grad()  #zero gradients\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_FG_7ag2sW8"
   },
   "outputs": [],
   "source": [
    "def get_train_step(model):\n",
    "    \"\"\"\n",
    "    Get train step\n",
    "    :param model: model\n",
    "    :return: train_step\n",
    "    \"\"\"\n",
    "\n",
    "    loss_fn = BCEWithLogitsLoss()  #binary cross entropy with sigmoid, so no need to use sigmoid in the model\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    train_step = make_train_step(model, optimizer, loss_fn)\n",
    "\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TRAINING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNFixOJyynlw"
   },
   "outputs": [],
   "source": [
    "def save_weights(model, filename='best_weight.pt'):\n",
    "    \"\"\"\n",
    "    Save model weights\n",
    "    :param model: model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    color = \"\\033[0;92m\" if 'best' in filename else '\\033[0;93m'\n",
    "    torch.save(model.state_dict(), ROOT_DIR + filename)\n",
    "    print(color+'Model weights saved at {}'.format(ROOT_DIR + filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, batch=16, n_epochs=15, early_stopping_tolerance=3,\n",
    "          early_stopping_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Train model\n",
    "    :param model: model\n",
    "    :param batch: batch size\n",
    "    :param n_epochs: number of epochs\n",
    "    :param early_stopping_tolerance: early stopping tolerance\n",
    "    :param early_stopping_threshold: early stopping threshold\n",
    "    :return:None\n",
    "    \"\"\"\n",
    "    train_step = get_train_step(model)\n",
    "    train_dataloader, val_dataloader = get_dataloaders(batch_size=batch)\n",
    "\n",
    "    loss_fn = BCEWithLogitsLoss()\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for i, data in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):  #iterate ove batches\n",
    "            x_batch, y_batch = data\n",
    "            x_batch = x_batch.to(device)  #move to gpu\n",
    "            y_batch = y_batch.unsqueeze(1).float()  #convert target to same nn output shape\n",
    "            y_batch = y_batch.to(device)  #move to gpu\n",
    "\n",
    "            loss = train_step(x_batch, y_batch)\n",
    "            epoch_loss += loss / len(train_dataloader)\n",
    "            losses.append(loss)\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss)\n",
    "        print('\\nEpoch : {}, train loss : {}'.format(epoch + 1, epoch_loss))\n",
    "\n",
    "        #validation doesnt requires gradient\n",
    "        with torch.no_grad():\n",
    "            cum_loss = 0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for x_batch, y_batch in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.unsqueeze(1).float()  #convert target to same nn output shape\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                #model to eval mode\n",
    "                model.eval()\n",
    "\n",
    "                yhat = model(x_batch)\n",
    "                val_loss = loss_fn(yhat, y_batch).item()\n",
    "                cum_loss += val_loss / len(val_dataloader)\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "                predicted = torch.round(torch.sigmoid(yhat))\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "            epoch_test_losses.append(cum_loss)\n",
    "            print('Epoch : {}, val loss : {}'.format(epoch + 1, cum_loss))\n",
    "            print('Accuracy of the model on the validation set: %.2f%%' % (100 * correct / total))\n",
    "\n",
    "            best_loss = min(epoch_test_losses)\n",
    "\n",
    "            #save best model\n",
    "            if cum_loss <= best_loss:\n",
    "                best_model_wts = model.state_dict()\n",
    "                save_weights(model)\n",
    "            else:\n",
    "                save_weights(model, filename='last_weight.pt')\n",
    "\n",
    "            if cum_loss > best_loss:\n",
    "                early_stopping_counter +=1 # add counter\n",
    "            else:\n",
    "                early_stopping_counter = 0 # reset counter\n",
    "\n",
    "            if (early_stopping_counter >= early_stopping_tolerance) or (best_loss <= early_stopping_threshold):\n",
    "                print(\"/nTerminating: early stopping\")\n",
    "                break  #terminate training\n",
    "\n",
    "    model.load_state_dict(torch.load(ROOT_DIR+\"best_weight.pt\"))\n",
    "    print(f'Finished Training, best loss : {best_loss}, best weights loaded in model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TESTING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "361ZTugaqQsD",
    "outputId": "db2f6ca3-9954-4174-acde-818127081ae0"
   },
   "outputs": [],
   "source": [
    "def check_val(model):\n",
    "    \"\"\"\n",
    "    Check validation loss and accuracy after training\n",
    "    :param model: model\n",
    "    :param val_dataloader: validation dataloader\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    _, val_dataloader = get_dataloaders()\n",
    "    loss_fn = BCEWithLogitsLoss()\n",
    "    with torch.no_grad():\n",
    "        cum_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for x_batch, y_batch in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.unsqueeze(1).float()  #convert target to same nn output shape\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            #model to eval mode\n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_batch)\n",
    "            val_loss = loss_fn(yhat, y_batch).item()\n",
    "            cum_loss += val_loss / len(val_dataloader)\n",
    "\n",
    "            predicted = torch.round(torch.sigmoid(yhat))\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        print('Accuracy of the model on the validation set: %.2f%%' % (100 * correct / total))\n",
    "        print('Loss of the model on the validation set: %.2f' % (cum_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "Ee5k12L22wQ2",
    "outputId": "f9051364-3dd5-411f-f4a8-39a7cea64309"
   },
   "outputs": [],
   "source": [
    "def predict(model, filename_='classifier.csv'):\n",
    "    \"\"\"\n",
    "    Predict test data\n",
    "    :param model: model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    _, _, test_transforms = get_transformers()\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(TEST_DIR)):\n",
    "        img = Image.open(os.path.join(TEST_DIR, filename)).convert('RGB')\n",
    "        img = test_transforms(img)\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            predicted = torch.round(torch.sigmoid(output))\n",
    "            confidence = torch.sigmoid(output)\n",
    "            results.append((filename.split('.')[0], confidence.item()))\n",
    "\n",
    "    pred_classifier = pd.DataFrame(results, columns=['image_id', 'target'])\n",
    "    pred_classifier.to_csv(filename_, index=False)\n",
    "    display(pred_classifier.head())\n",
    "    print('Predictions saved at {}'.format(ROOT_DIR + filename_))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RUN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DOWNLOAD DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11NHcdE8Z3u7",
    "outputId": "43a797e4-7e5c-472e-c1d1-97d8b07efec9"
   },
   "outputs": [],
   "source": [
    "download_data(image_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PREPROCESS DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_preparation()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MODEL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = get_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TRAIN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model, batch=16, n_epochs=15, early_stopping_tolerance=5, early_stopping_threshold=0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CHECK VAL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "check_val(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PREDICT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FINE TUNING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### FINE TUNING LOOP THROUGH MODELS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = [\"EfficientNet_B7\", \"EfficientNet_B6\", \"EfficientNet_B5\", \"EfficientNet_B4\", \"EfficientNet_B3\", \"resnet18\", \"resnet50\", \"resnet101\", \"resnet152\", \"densenet121\", \"densenet169\", \"densenet201\"]\n",
    "for model_ in models:\n",
    "    print(f\"Training {model_}\")\n",
    "    model = get_model(model_)\n",
    "    train(model, batch=16, n_epochs=10, early_stopping_tolerance=3, early_stopping_threshold=0.01)\n",
    "    check_val(model)\n",
    "    predict(model, filename=f\"{model_}.csv\")\n",
    "    print(f\"Finished training {model_}\")\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### FINE TUNING LOOP THROUGH BATCH SIZE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for batch in [8, 16, 32, 64]:\n",
    "    print(f\"Training with batch size {batch}\")\n",
    "    model = get_model()\n",
    "    train(model, batch=batch, n_epochs=10, early_stopping_tolerance=3, early_stopping_threshold=0.01)\n",
    "    check_val(model)\n",
    "    predict(model, filename=f\"batch_{batch}.csv\")\n",
    "    print(f\"Finished training with batch size {batch}\")\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### FINE TUNING LOOP THROUGH EPOCHS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n_epochs in [5, 10, 15, 20]:\n",
    "    print(f\"Training with {n_epochs} epochs\")\n",
    "    model = get_model()\n",
    "    train(model, batch=16, n_epochs=n_epochs, early_stopping_tolerance=3, early_stopping_threshold=0.01)\n",
    "    check_val(model)\n",
    "    predict(model, filename=f\"epochs_{n_epochs}.csv\")\n",
    "    print(f\"Finished training with {n_epochs} epochs\")\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "098cf4d874974689a4761fc62da322a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89ceec7431be49d3b71cc9cfc637df42",
       "IPY_MODEL_5a9e68d3a3f04a3a803b0392385c48f1",
       "IPY_MODEL_8fc24e6961764b56996a372addda245d"
      ],
      "layout": "IPY_MODEL_7b9c359627134a20ad0bba34f0abaa85"
     }
    },
    "89ceec7431be49d3b71cc9cfc637df42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79d9a22980534d04b640db8ac60b9a2e",
      "placeholder": "​",
      "style": "IPY_MODEL_f83b79b96d354879bdd3f626243c75da",
      "value": "100%"
     }
    },
    "5a9e68d3a3f04a3a803b0392385c48f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7253198b1d5e466d9556a54edf1638c3",
      "max": 267046505,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_859351ad2be8429ea5166518383c62a3",
      "value": 267046505
     }
    },
    "8fc24e6961764b56996a372addda245d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1d8d07d95384257a6461dc8ac0ad6bb",
      "placeholder": "​",
      "style": "IPY_MODEL_a861b94f20294abb9e8439f19ffbee9f",
      "value": " 255M/255M [00:01&lt;00:00, 237MB/s]"
     }
    },
    "7b9c359627134a20ad0bba34f0abaa85": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79d9a22980534d04b640db8ac60b9a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f83b79b96d354879bdd3f626243c75da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7253198b1d5e466d9556a54edf1638c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "859351ad2be8429ea5166518383c62a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1d8d07d95384257a6461dc8ac0ad6bb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a861b94f20294abb9e8439f19ffbee9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
