{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IMPORTS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0W2BOLFzwpKD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CONFIG\n",
    "\n",
    "#### Project Config:\n",
    "\n",
    "- root_dir :\n",
    "    - train.csv\n",
    "    - test.csv\n",
    "    - train_meta.csv\n",
    "    - kaggle.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ROOT_DIR = '/kaggle/'\n",
    "TRAIN_DIR = ROOT_DIR + 'data/train/'\n",
    "TEST_DIR = ROOT_DIR + 'data/test/'\n",
    "TRAIN_CSV = ROOT_DIR + 'train.csv'\n",
    "TRAIN_META_CSV = ROOT_DIR + 'train_meta.csv'\n",
    "TEST_CSV = ROOT_DIR + 'test.csv'\n",
    "CLASSIFIER = ROOT_DIR + 'classifier/'\n",
    "CLASSIFIER_TRAIN = CLASSIFIER + 'train/'\n",
    "CLASSIFIER_VAL = CLASSIFIER + 'val/'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## /!\\ TO TRIGGER IF RUNNING ON KAGGLE ONLY /!\\"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m     get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpwd\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     17\u001B[0m     get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mls\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 19\u001B[0m \u001B[43mis_kaggle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[18], line 11\u001B[0m, in \u001B[0;36mis_kaggle\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_kaggle\u001B[39m():\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    If the code is running on Kaggle take custom data from private repo and copy to root directory\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    /!\\ On Kaggle I have a private dataset (data-ai) containing: /!\\\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03m    You can use your own data or download from Kaggle, just change the paths\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     \u001B[43mos\u001B[49m\u001B[38;5;241m.\u001B[39mchdir(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     12\u001B[0m     get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcp input/data-ai/kaggle.json .\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     13\u001B[0m     get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcp input/data-ai/test.csv .\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def is_kaggle():\n",
    "    \"\"\"\n",
    "    If the code is running on Kaggle take custom data from private repo and copy to root directory\n",
    "    /!\\ On Kaggle I have a private dataset (data-ai) containing: /!\\\n",
    "    - train.csv\n",
    "    - test.csv\n",
    "    - train_meta.csv\n",
    "    - kaggle.json\n",
    "    You can use your own data or download from Kaggle, just change the paths\n",
    "    \"\"\"\n",
    "    os.chdir('..')\n",
    "    !cp input/data-ai/kaggle.json .\n",
    "    !cp input/data-ai/test.csv .\n",
    "    !cp input/data-ai/train.csv .\n",
    "    !cp input/data-ai/train_meta.csv .\n",
    "    !pwd\n",
    "    !ls\n",
    "\n",
    "is_kaggle()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FUNCTIONS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DONWLOAD DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download_data(image_size=512):\n",
    "    \"\"\"\n",
    "    Download data from Kaggle\n",
    "    :param image_size: 256, 512 or 1024\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    !pip install -q kaggle\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    if image_size == 256:\n",
    "        !kaggle datasets download -d xhlulu/vinbigdata-chest-xray-resized-png-256x256\n",
    "        !unzip vinbigdata-chest-xray-resized-png-256x256.zip -d data\n",
    "    elif image_size == 512:\n",
    "        !kaggle datasets download -d xhlulu/vinbigdata\n",
    "        !unzip vinbigdata.zip -d data\n",
    "    elif image_size == 1024:\n",
    "        !kaggle datasets download -d xhlulu/vinbigdata-chest-xray-resized-png-1024x1024\n",
    "        !unzip vinbigdata-chest-xray-resized-png-1024x1024.zip -d data\n",
    "    else:\n",
    "        print(\"Image size not supported\")\n",
    "\n",
    "    print(f\"Number of train files: {len(glob(TRAIN_DIR + '*.png'))}\")\n",
    "    print(f\"Number of test files: {len(glob(TEST_DIR + '*.png'))}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DIRECTORIES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBv263mtzyCQ"
   },
   "outputs": [],
   "source": [
    "def create_directories():\n",
    "    \"\"\"\n",
    "    Create directories for the project\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    os.mkdir('classifier')\n",
    "    os.chdir('classifier')\n",
    "    os.mkdir('train')\n",
    "    os.mkdir('val')\n",
    "    os.chdir('train')\n",
    "    os.mkdir('finding')\n",
    "    os.mkdir('no_finding')\n",
    "    os.chdir('../val')\n",
    "    os.mkdir('finding')\n",
    "    os.mkdir('no_finding')\n",
    "    os.chdir('..')\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DATA PREPARATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_image_path(df_train):\n",
    "    \"\"\"\n",
    "    Add image path to train dataframe\n",
    "    :param df_train: Dataframe with train data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    df_train['image_path'] = TRAIN_DIR + df_train.image_id + '.png'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_fold_split(df_train, n_splits=6):\n",
    "    \"\"\"\n",
    "    Split train data into train and validation sets\n",
    "    :param df_train: Dataframe with train data\n",
    "    :param n_splits: Number of splits\n",
    "    :return: train_files, val_files\n",
    "    \"\"\"\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    df_train['fold'] = -1\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(df_train, groups=df_train.image_id.tolist())):\n",
    "        df_train.loc[val_idx, 'fold'] = fold\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    val_files += list(df_train[df_train.fold == fold].image_path.unique())\n",
    "    train_files += list(df_train[df_train.fold != fold].image_path.unique())\n",
    "    print(f\"Train: {len(train_files)} - Val: {len(val_files)}\")\n",
    "    return train_files, val_files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOuzndbB13sj",
    "outputId": "095eb2d3-84a0-4a25-ad38-426ef45277ac"
   },
   "outputs": [],
   "source": [
    "def copy_files(df_train):\n",
    "    \"\"\"\n",
    "    Copy files to train and validation directories\n",
    "    :param df_train: Dataframe with train data\n",
    "    :param classifier_train: Path to train directory\n",
    "    :param classifier_val: Path to validation directory\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for i, row in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "        if row.fold == 1:\n",
    "            if row['class_id'] == 14:\n",
    "                shutil.copyfile(row.image_path, CLASSIFIER_VAL + 'no_finding/' + row.image_id + '.png')\n",
    "            else:\n",
    "                shutil.copyfile(row.image_path, CLASSIFIER_VAL + 'finding/' + row.image_id + '.png')\n",
    "        else:\n",
    "            if row['class_id'] == 14:\n",
    "                shutil.copyfile(row.image_path, CLASSIFIER_TRAIN + 'no_finding/' + row.image_id + '.png')\n",
    "            else:\n",
    "                shutil.copyfile(row.image_path, CLASSIFIER_TRAIN + 'finding/' + row.image_id + '.png')\n",
    "    print(f\"Train No Finding: {len([f for f in os.listdir(CLASSIFIER_TRAIN + 'no_finding/')])}\")\n",
    "    print(f\"Train Finding: {len([f for f in os.listdir(CLASSIFIER_TRAIN + 'finding/')])}\")\n",
    "    print(f\"Val No Finding: {len([f for f in os.listdir(CLASSIFIER_VAL + 'no_finding/')])}\")\n",
    "    print(f\"Val Finding: {len([f for f in os.listdir(CLASSIFIER_VAL + 'finding/')])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiMfU8lO17xi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "06321da0-6d37-4335-bf7f-5cfe46583663"
   },
   "outputs": [],
   "source": [
    "def data_preparation():\n",
    "    \"\"\"\n",
    "    Prepare data for training\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    create_directories()\n",
    "    df_train = pd.read_csv(TRAIN_CSV)\n",
    "    df_meta = pd.read_csv(TRAIN_META_CSV)\n",
    "    add_image_path(df_train)\n",
    "    train_files, val_files = get_fold_split(df_train)\n",
    "    print('Copy files...')\n",
    "    copy_files(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DATA AUGMENTATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_transformers():\n",
    "    \"\"\"\n",
    "    Get transformers for train and validation sets\n",
    "    :return: train_transforms, val_transforms, test_transforms\n",
    "    \"\"\"\n",
    "    train_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        # torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "        # torchvision.transforms.RandomRotation(10),\n",
    "        # torchvision.transforms.RandomApply([torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)], p=0.5),\n",
    "        # torchvision.transforms.RandomApply([torchvision.transforms.GaussianBlur(3, sigma=(0.1, 2.0))], p=0.5),\n",
    "        # torchvision.transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        torchvision.transforms.RandomAffine(degrees=10, translate=(0, 0.1), scale=(1, 1.10)),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    test_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return train_transforms, val_transforms, test_transforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QogSKcz-2nBH"
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size=16):\n",
    "    \"\"\"\n",
    "    Get dataloaders for train and validation sets\n",
    "    :return: train_dataloader, val_dataloader\n",
    "    \"\"\"\n",
    "    train_transforms, val_transforms, _ = get_transformers()\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=CLASSIFIER_TRAIN, transform=train_transforms)\n",
    "    val_dataset = datasets.ImageFolder(root=CLASSIFIER_VAL, transform=val_transforms)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2,\n",
    "                                                   pin_memory=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2,\n",
    "                                                 pin_memory=True)\n",
    "    print(f\"Train: {len(train_dataset)} - Val: {len(val_dataset)}\")\n",
    "    print('Idx to class:', train_dataset.class_to_idx)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MODEL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Download model and replace last layer\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    model = torchvision.models.efficientnet_b7(weights=torchvision.models.EfficientNet_B7_Weights)\n",
    "    # Get the last layer of the model\n",
    "    last_layer = list(model.children())[-1]\n",
    "    # Get the sub-layer of the last layer\n",
    "    last_layer_layer = list(last_layer.children())[-1]\n",
    "    # Get the number of input features for the sub-layer\n",
    "    num_features = last_layer_layer.in_features\n",
    "    # Replace the sub-layer with a linear layer with 1 output\n",
    "    last_layer[-1] = torch.nn.Linear(num_features, 1)\n",
    "    # Print the updated model architecture\n",
    "    # print(model)\n",
    "    if torch.cuda.device_count() > 1: # check if multiple GPUs are available\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = torch.nn.DataParallel(model) # make parallel\n",
    "    model = model.to(device)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc4wjRDA2qaO"
   },
   "outputs": [],
   "source": [
    "def make_train_step(model, optimizer, loss_fn):\n",
    "    \"\"\"\n",
    "    Builds function that performs a step in the train loop\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    :param loss_fn: loss function\n",
    "    :return: train_step\n",
    "    \"\"\"\n",
    "    def train_step(x, y):\n",
    "        \"\"\"\n",
    "        Performs a train step\n",
    "        :param x: input\n",
    "        :param y: target\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        yhat = model(x)  # Make prediction\n",
    "        model.train()  #enter train mode\n",
    "        loss = loss_fn(yhat, y)  #compute loss\n",
    "        loss.backward()  # backpropagation\n",
    "        optimizer.step()  #update weights\n",
    "        optimizer.zero_grad()  #zero gradients\n",
    "\n",
    "        return loss\n",
    "\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_FG_7ag2sW8"
   },
   "outputs": [],
   "source": [
    "def get_train_step(model):\n",
    "    \"\"\"\n",
    "    Get train step\n",
    "    :param model: model\n",
    "    :return: train_step\n",
    "    \"\"\"\n",
    "\n",
    "    loss_fn = BCEWithLogitsLoss()  #binary cross entropy with sigmoid, so no need to use sigmoid in the model\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    train_step = make_train_step(model, optimizer, loss_fn)\n",
    "\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TRAINING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "xnZyHJ8n2t3v",
    "outputId": "3bc38de1-5464-4d69-ed46-d4c3bfb0b27a"
   },
   "outputs": [],
   "source": [
    "def train(model, train_step, train_dataloader, val_dataloader, n_epochs=15, early_stopping_tolerance=3,\n",
    "          early_stopping_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Train model\n",
    "    :param model: model\n",
    "    :param train_step: train step\n",
    "    :param train_dataloader: train dataloader\n",
    "    :param val_dataloader: validation dataloader\n",
    "    :param n_epochs: number of epochs\n",
    "    :param early_stopping_tolerance: early stopping tolerance\n",
    "    :param early_stopping_threshold: early stopping threshold\n",
    "    :return:None\n",
    "    \"\"\"\n",
    "    loss_fn = BCEWithLogitsLoss()\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for i, data in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):  #iterate ove batches\n",
    "            x_batch, y_batch = data\n",
    "            x_batch = x_batch.to(device)  #move to gpu\n",
    "            y_batch = y_batch.unsqueeze(1).float()  #convert target to same nn output shape\n",
    "            y_batch = y_batch.to(device)  #move to gpu\n",
    "\n",
    "            loss = train_step(x_batch, y_batch)\n",
    "            epoch_loss += loss / len(train_dataloader)\n",
    "            losses.append(loss)\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss)\n",
    "        print('\\nEpoch : {}, train loss : {}'.format(epoch + 1, epoch_loss))\n",
    "\n",
    "        #validation doesnt requires gradient\n",
    "        with torch.no_grad():\n",
    "            cum_loss = 0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for x_batch, y_batch in val_dataloader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.unsqueeze(1).float()  #convert target to same nn output shape\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                #model to eval mode\n",
    "                model.eval()\n",
    "\n",
    "                yhat = model(x_batch)\n",
    "                val_loss = loss_fn(yhat, y_batch)\n",
    "                cum_loss += val_loss / len(val_dataloader)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                predicted = torch.round(torch.sigmoid(yhat))\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "            epoch_test_losses.append(cum_loss)\n",
    "            print('Epoch : {}, val loss : {}'.format(epoch + 1, cum_loss))\n",
    "            print('Accuracy of the model on the validation set: %.2f%%' % (100 * correct / total))\n",
    "\n",
    "            best_loss = min(epoch_test_losses)\n",
    "\n",
    "            #save best model\n",
    "            if cum_loss <= best_loss:\n",
    "                best_model_wts = model.state_dict()\n",
    "                print('SAVED')\n",
    "\n",
    "            if cum_loss > best_loss:\n",
    "                early_stopping_counter +=1 # add counter\n",
    "            else:\n",
    "                early_stopping_counter = 0 # reset counter\n",
    "\n",
    "            if (early_stopping_counter >= early_stopping_tolerance) or (best_loss <= early_stopping_threshold):\n",
    "                print(\"/nTerminating: early stopping\")\n",
    "                break  #terminate training\n",
    "\n",
    "    model.load_state_dict(best_model_wts) #load best model\n",
    "    print(f'Finished Training, best loss : {best_loss}, best weights loaded in model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UNFixOJyynlw"
   },
   "outputs": [],
   "source": [
    "def save_weights(model):\n",
    "    \"\"\"\n",
    "    Save model weights\n",
    "    :param model: model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), ROOT_DIR + 'best_weight.pt')\n",
    "    print('Model weights saved at {}'.format(ROOT_DIR + 'best_weight.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TESTING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "361ZTugaqQsD",
    "outputId": "db2f6ca3-9954-4174-acde-818127081ae0"
   },
   "outputs": [],
   "source": [
    "def check_val(model, val_dataloader):\n",
    "    \"\"\"\n",
    "    Check validation loss and accuracy after training\n",
    "    :param model: model\n",
    "    :param val_dataloader: validation dataloader\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    loss_fn = BCEWithLogitsLoss()\n",
    "    with torch.no_grad():\n",
    "        cum_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for x_batch, y_batch in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.unsqueeze(1).float()  #convert target to same nn output shape\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            #model to eval mode\n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_batch)\n",
    "            val_loss = loss_fn(yhat, y_batch)\n",
    "            cum_loss += val_loss / len(val_dataloader)\n",
    "\n",
    "            predicted = torch.round(torch.sigmoid(yhat))\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        print('Accuracy of the model on the validation set: %.2f%%' % (100 * correct / total))\n",
    "        print('Loss of the model on the validation set: %.2f' % (cum_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "Ee5k12L22wQ2",
    "outputId": "f9051364-3dd5-411f-f4a8-39a7cea64309"
   },
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    \"\"\"\n",
    "    Predict test data\n",
    "    :param model: model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    _, _, test_transforms = get_transformers()\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(TEST_DIR)):\n",
    "        img = Image.open(os.path.join(TEST_DIR, filename)).convert('RGB')\n",
    "        img = test_transforms(img)\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            predicted = torch.round(torch.sigmoid(output))\n",
    "            confidence = torch.sigmoid(output)\n",
    "            results.append((filename.split('.')[0], confidence.item()))\n",
    "\n",
    "    pred_classifier = pd.DataFrame(results, columns=['image_id', 'target'])\n",
    "    pred_classifier.to_csv('classifier.csv', index=False)\n",
    "    display(pred_classifier.head())\n",
    "    print('Predictions saved at {}'.format(ROOT_DIR + 'classifier.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RUN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DOWNLOAD DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11NHcdE8Z3u7",
    "outputId": "43a797e4-7e5c-472e-c1d1-97d8b07efec9"
   },
   "outputs": [],
   "source": [
    "download_data(image_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PREPROCESS DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_preparation()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MODEL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mget_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m train_step \u001B[38;5;241m=\u001B[39m get_train_step(model)\n\u001B[1;32m      3\u001B[0m train_dataloader, val_dataloader \u001B[38;5;241m=\u001B[39m get_dataloaders(batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m)\n",
      "Cell \u001B[0;32mIn[23], line 6\u001B[0m, in \u001B[0;36mget_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_model\u001B[39m():\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    Download model and replace last layer\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    :return: model\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mtorchvision\u001B[49m\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mefficientnet_b7(weights\u001B[38;5;241m=\u001B[39mtorchvision\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mEfficientNet_B7_Weights)\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# Get the last layer of the model\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     last_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(model\u001B[38;5;241m.\u001B[39mchildren())[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "train_step = get_train_step(model)\n",
    "train_dataloader, val_dataloader = get_dataloaders(batch_size=16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TRAIN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model, train_step, train_dataloader, val_dataloader, n_epochs=15, early_stopping_tolerance=5, early_stopping_threshold=0.01)\n",
    "save_weights(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CHECK VAL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "check_val(model, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PREDICT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "098cf4d874974689a4761fc62da322a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89ceec7431be49d3b71cc9cfc637df42",
       "IPY_MODEL_5a9e68d3a3f04a3a803b0392385c48f1",
       "IPY_MODEL_8fc24e6961764b56996a372addda245d"
      ],
      "layout": "IPY_MODEL_7b9c359627134a20ad0bba34f0abaa85"
     }
    },
    "89ceec7431be49d3b71cc9cfc637df42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79d9a22980534d04b640db8ac60b9a2e",
      "placeholder": "​",
      "style": "IPY_MODEL_f83b79b96d354879bdd3f626243c75da",
      "value": "100%"
     }
    },
    "5a9e68d3a3f04a3a803b0392385c48f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7253198b1d5e466d9556a54edf1638c3",
      "max": 267046505,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_859351ad2be8429ea5166518383c62a3",
      "value": 267046505
     }
    },
    "8fc24e6961764b56996a372addda245d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1d8d07d95384257a6461dc8ac0ad6bb",
      "placeholder": "​",
      "style": "IPY_MODEL_a861b94f20294abb9e8439f19ffbee9f",
      "value": " 255M/255M [00:01&lt;00:00, 237MB/s]"
     }
    },
    "7b9c359627134a20ad0bba34f0abaa85": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79d9a22980534d04b640db8ac60b9a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f83b79b96d354879bdd3f626243c75da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7253198b1d5e466d9556a54edf1638c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "859351ad2be8429ea5166518383c62a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1d8d07d95384257a6461dc8ac0ad6bb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a861b94f20294abb9e8439f19ffbee9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
