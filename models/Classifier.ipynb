{
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "098cf4d874974689a4761fc62da322a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89ceec7431be49d3b71cc9cfc637df42",
       "IPY_MODEL_5a9e68d3a3f04a3a803b0392385c48f1",
       "IPY_MODEL_8fc24e6961764b56996a372addda245d"
      ],
      "layout": "IPY_MODEL_7b9c359627134a20ad0bba34f0abaa85"
     }
    },
    "89ceec7431be49d3b71cc9cfc637df42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79d9a22980534d04b640db8ac60b9a2e",
      "placeholder": "​",
      "style": "IPY_MODEL_f83b79b96d354879bdd3f626243c75da",
      "value": "100%"
     }
    },
    "5a9e68d3a3f04a3a803b0392385c48f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7253198b1d5e466d9556a54edf1638c3",
      "max": 267046505,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_859351ad2be8429ea5166518383c62a3",
      "value": 267046505
     }
    },
    "8fc24e6961764b56996a372addda245d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1d8d07d95384257a6461dc8ac0ad6bb",
      "placeholder": "​",
      "style": "IPY_MODEL_a861b94f20294abb9e8439f19ffbee9f",
      "value": " 255M/255M [00:01&lt;00:00, 237MB/s]"
     }
    },
    "7b9c359627134a20ad0bba34f0abaa85": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79d9a22980534d04b640db8ac60b9a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f83b79b96d354879bdd3f626243c75da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7253198b1d5e466d9556a54edf1638c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "859351ad2be8429ea5166518383c62a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1d8d07d95384257a6461dc8ac0ad6bb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a861b94f20294abb9e8439f19ffbee9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# CLASSIFIER",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## IMPORTS",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport torch\nimport torchvision\nfrom torchvision import datasets\nimport os\nfrom glob import glob\nimport pandas as pd\nfrom tqdm import tqdm\nimport shutil\nfrom sklearn.model_selection import GroupKFold\nfrom torch.nn.modules.loss import BCEWithLogitsLoss\nfrom torch.optim import lr_scheduler\nimport numpy as np\nfrom PIL import Image\nimport seaborn as sns\nimport math",
   "metadata": {
    "id": "0W2BOLFzwpKD",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:15.494150Z",
     "iopub.execute_input": "2023-03-27T07:23:15.494849Z",
     "iopub.status.idle": "2023-03-27T07:23:20.818810Z",
     "shell.execute_reply.started": "2023-03-27T07:23:15.494809Z",
     "shell.execute_reply": "2023-03-27T07:23:20.817720Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## CONFIG\n\n#### Project Config:\n\n- root_dir :\n    - train.csv\n    - kaggle.json",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "ROOT_DIR = '/kaggle/'  # '/content/' if Google Colab, '/kaggle/' if Kaggle\nTRAIN_DIR = ROOT_DIR + 'data/train/'\nTEST_DIR = ROOT_DIR + 'data/test/'\nTRAIN_CSV = ROOT_DIR + 'train.csv'\nTRAIN_META_CSV = ROOT_DIR + 'train_meta.csv'\nTEST_CSV = ROOT_DIR + 'test.csv'\nCLASSIFIER = ROOT_DIR + 'classifier/'\nCLASSIFIER_TRAIN = CLASSIFIER + 'train/'\nCLASSIFIER_VAL = CLASSIFIER + 'val/'\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:20.820663Z",
     "iopub.execute_input": "2023-03-27T07:23:20.821671Z",
     "iopub.status.idle": "2023-03-27T07:23:20.929596Z",
     "shell.execute_reply.started": "2023-03-27T07:23:20.821632Z",
     "shell.execute_reply": "2023-03-27T07:23:20.926191Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## /!\\ TO TRIGGER IF RUNNING ON KAGGLE ONLY /!\\",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def is_kaggle():\n    \"\"\"\n    If the code is running on Kaggle take custom data from private repo and copy to root directory\n    /!\\ On Kaggle I have a private dataset (data-ai) containing: /!\\\n    - train.csv\n    - test.csv\n    - train_meta.csv\n    - kaggle.json\n    You can use your own data or download from Kaggle, just change the paths\n    \"\"\"\n    os.chdir('..')\n    !cp input/data-ai/kaggle.json .\n    !cp input/data-ai/test.csv .\n    !cp input/data-ai/train.csv .\n    !cp input/data-ai/train_meta.csv .\n    !pwd\n    !ls\n\nis_kaggle()",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:20.931068Z",
     "iopub.execute_input": "2023-03-27T07:23:20.931453Z",
     "iopub.status.idle": "2023-03-27T07:23:26.925532Z",
     "shell.execute_reply.started": "2023-03-27T07:23:20.931423Z",
     "shell.execute_reply": "2023-03-27T07:23:26.924310Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle\ninput  kaggle.json  lib  test.csv  train.csv  train_meta.csv  working\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## FUNCTIONS",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### DONWLOAD DATA",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def download_data(image_size=512):\n    \"\"\"\n    Download data from Kaggle\n    :param image_size: 256, 512 or 1024\n    :return: None\n    \"\"\"\n    !pip install -q kaggle\n    !mkdir -p ~/.kaggle\n    !cp kaggle.json ~/.kaggle/\n    !chmod 600 ~/.kaggle/kaggle.json\n    if image_size == 256:\n        !kaggle datasets download -d xhlulu/vinbigdata-chest-xray-resized-png-256x256\n        !unzip vinbigdata-chest-xray-resized-png-256x256.zip -d data\n    elif image_size == 512:\n        !kaggle datasets download -d xhlulu/vinbigdata\n        !unzip vinbigdata.zip -d data\n    elif image_size == 1024:\n        !kaggle datasets download -d xhlulu/vinbigdata-chest-xray-resized-png-1024x1024\n        !unzip vinbigdata-chest-xray-resized-png-1024x1024.zip -d data\n    else:\n        print(\"Image size not supported\")\n\n    print(f\"Number of train files: {len(glob(TRAIN_DIR + '*.png'))}\")\n    print(f\"Number of test files: {len(glob(TEST_DIR + '*.png'))}\")",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:26.929088Z",
     "iopub.execute_input": "2023-03-27T07:23:26.929833Z",
     "iopub.status.idle": "2023-03-27T07:23:26.958924Z",
     "shell.execute_reply.started": "2023-03-27T07:23:26.929774Z",
     "shell.execute_reply": "2023-03-27T07:23:26.957689Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### DIRECTORIES",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_directories():\n    \"\"\"\n    Create directories for the project\n    :return: None\n    \"\"\"\n    os.mkdir('classifier')\n    os.chdir('classifier')\n    os.mkdir('train')\n    os.mkdir('val')\n    os.chdir('train')\n    os.mkdir('finding')\n    os.mkdir('no_finding')\n    os.chdir('../val')\n    os.mkdir('finding')\n    os.mkdir('no_finding')\n    os.chdir('..')\n    os.chdir('..')",
   "metadata": {
    "id": "yBv263mtzyCQ",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:26.960934Z",
     "iopub.execute_input": "2023-03-27T07:23:26.961810Z",
     "iopub.status.idle": "2023-03-27T07:23:26.971178Z",
     "shell.execute_reply.started": "2023-03-27T07:23:26.961772Z",
     "shell.execute_reply": "2023-03-27T07:23:26.970056Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### DATA PREPARATION",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def add_image_path(df_train):\n    \"\"\"\n    Add image path to train dataframe\n    :param df_train: Dataframe with train data\n    :return: None\n    \"\"\"\n    df_train['image_path'] = TRAIN_DIR + df_train.image_id + '.png'",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:26.972841Z",
     "iopub.execute_input": "2023-03-27T07:23:26.973097Z",
     "iopub.status.idle": "2023-03-27T07:23:26.981415Z",
     "shell.execute_reply.started": "2023-03-27T07:23:26.973072Z",
     "shell.execute_reply": "2023-03-27T07:23:26.980401Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_fold_split(df_train, n_splits=6):\n    \"\"\"\n    Split train data into train and validation sets\n    :param df_train: Dataframe with train data\n    :param n_splits: Number of splits\n    :return: train_files, val_files\n    \"\"\"\n    gkf = GroupKFold(n_splits=n_splits)\n    df_train['fold'] = -1\n    for fold, (train_idx, val_idx) in enumerate(gkf.split(df_train, groups=df_train.image_id.tolist())):\n        df_train.loc[val_idx, 'fold'] = fold\n    train_files = []\n    val_files = []\n    val_files += list(df_train[df_train.fold == fold].image_path.unique())\n    train_files += list(df_train[df_train.fold != fold].image_path.unique())\n    print(f\"Train: {len(train_files)} - Val: {len(val_files)}\")\n    return train_files, val_files",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:26.982818Z",
     "iopub.execute_input": "2023-03-27T07:23:26.983573Z",
     "iopub.status.idle": "2023-03-27T07:23:26.992084Z",
     "shell.execute_reply.started": "2023-03-27T07:23:26.983537Z",
     "shell.execute_reply": "2023-03-27T07:23:26.991060Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def copy_files(df_train):\n    \"\"\"\n    Copy files to train and validation directories\n    :param df_train: Dataframe with train data\n    :param classifier_train: Path to train directory\n    :param classifier_val: Path to validation directory\n    :return: None\n    \"\"\"\n    for i, row in tqdm(df_train.iterrows(), total=len(df_train)):\n        if row.fold == 1:\n            if row['class_id'] == 14:\n                shutil.copyfile(row.image_path, CLASSIFIER_VAL + 'no_finding/' + row.image_id + '.png')\n            else:\n                shutil.copyfile(row.image_path, CLASSIFIER_VAL + 'finding/' + row.image_id + '.png')\n        else:\n            if row['class_id'] == 14:\n                shutil.copyfile(row.image_path, CLASSIFIER_TRAIN + 'no_finding/' + row.image_id + '.png')\n            else:\n                shutil.copyfile(row.image_path, CLASSIFIER_TRAIN + 'finding/' + row.image_id + '.png')\n    print(f\"Train No Finding: {len([f for f in os.listdir(CLASSIFIER_TRAIN + 'no_finding/')])}\")\n    print(f\"Train Finding: {len([f for f in os.listdir(CLASSIFIER_TRAIN + 'finding/')])}\")\n    print(f\"Val No Finding: {len([f for f in os.listdir(CLASSIFIER_VAL + 'no_finding/')])}\")\n    print(f\"Val Finding: {len([f for f in os.listdir(CLASSIFIER_VAL + 'finding/')])}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOuzndbB13sj",
    "outputId": "095eb2d3-84a0-4a25-ad38-426ef45277ac",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:26.993558Z",
     "iopub.execute_input": "2023-03-27T07:23:26.994241Z",
     "iopub.status.idle": "2023-03-27T07:23:27.008037Z",
     "shell.execute_reply.started": "2023-03-27T07:23:26.994156Z",
     "shell.execute_reply": "2023-03-27T07:23:27.007057Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def data_preparation():\n    \"\"\"\n    Prepare data for training\n    :return: None\n    \"\"\"\n    create_directories()\n    df_train = pd.read_csv(TRAIN_CSV)\n    df_meta = pd.read_csv(TRAIN_META_CSV)\n    add_image_path(df_train)\n    train_files, val_files = get_fold_split(df_train)\n    print('Copy files...')\n    copy_files(df_train)",
   "metadata": {
    "id": "KiMfU8lO17xi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "06321da0-6d37-4335-bf7f-5cfe46583663",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:27.009543Z",
     "iopub.execute_input": "2023-03-27T07:23:27.010485Z",
     "iopub.status.idle": "2023-03-27T07:23:27.018545Z",
     "shell.execute_reply.started": "2023-03-27T07:23:27.010444Z",
     "shell.execute_reply": "2023-03-27T07:23:27.017410Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### DATA AUGMENTATION",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_transformers():\n    \"\"\"\n    Get transformers for train and validation sets\n    :return: train_transforms, val_transforms, test_transforms\n    \"\"\"\n    train_transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize(256),\n        # torchvision.transforms.RandomHorizontalFlip(p=0.5),\n        # torchvision.transforms.RandomVerticalFlip(p=0.5),\n        # torchvision.transforms.RandomRotation(10),\n        # torchvision.transforms.RandomApply([torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)], p=0.5),\n        # torchvision.transforms.RandomApply([torchvision.transforms.GaussianBlur(3, sigma=(0.1, 2.0))], p=0.5),\n        # torchvision.transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n        torchvision.transforms.RandomAffine(degrees=10, translate=(0, 0.1), scale=(1, 1.10)),\n        torchvision.transforms.CenterCrop(224),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    val_transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize(256),\n        torchvision.transforms.CenterCrop(224),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    test_transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize(256),\n        torchvision.transforms.CenterCrop(224),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    return train_transforms, val_transforms, test_transforms",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:27.022595Z",
     "iopub.execute_input": "2023-03-27T07:23:27.023876Z",
     "iopub.status.idle": "2023-03-27T07:23:27.033569Z",
     "shell.execute_reply.started": "2023-03-27T07:23:27.023836Z",
     "shell.execute_reply": "2023-03-27T07:23:27.032614Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_dataloaders(batch_size=16):\n    \"\"\"\n    Get dataloaders for train and validation sets\n    :return: train_dataloader, val_dataloader\n    \"\"\"\n    train_transforms, val_transforms, _ = get_transformers()\n\n    train_dataset = datasets.ImageFolder(root=CLASSIFIER_TRAIN, transform=train_transforms)\n    val_dataset = datasets.ImageFolder(root=CLASSIFIER_VAL, transform=val_transforms)\n\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2,\n                                                   pin_memory=True)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2,\n                                                 pin_memory=True)\n    print(f\"Train: {len(train_dataset)} - Val: {len(val_dataset)}\")\n    print('Idx to class:', train_dataset.class_to_idx)\n\n    return train_dataloader, val_dataloader",
   "metadata": {
    "id": "QogSKcz-2nBH",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:27.036861Z",
     "iopub.execute_input": "2023-03-27T07:23:27.037124Z",
     "iopub.status.idle": "2023-03-27T07:23:27.045720Z",
     "shell.execute_reply.started": "2023-03-27T07:23:27.037100Z",
     "shell.execute_reply": "2023-03-27T07:23:27.044606Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### MODEL",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_model(model_='EfficientNet_B7'):\n    \"\"\"\n    Download model and replace last layer\n    :return: model\n    \"\"\"\n    if model_=='EfficientNet_B7':\n        model = torchvision.models.efficientnet_b7(weights=torchvision.models.EfficientNet_B7_Weights)\n        last_layer = list(model.children())[-1] # Get the last layer of the model\n        last_layer_layer = list(last_layer.children())[-1] # Get the sub-layer of the last layer\n        num_features = last_layer_layer.in_features # Get the number of input features for the sub-layer\n        last_layer[-1] = torch.nn.Linear(num_features, 1) # Replace the sub-layer with a linear layer with 1 output\n        # print(model) # Print the updated model architecture\n    elif model_=='EfficientNet_B6':\n        model = torchvision.models.efficientnet_b6(weights=torchvision.models.EfficientNet_B6_Weights)\n        last_layer = list(model.children())[-1] # Get the last layer of the model\n        last_layer_layer = list(last_layer.children())[-1] # Get the sub-layer of the last layer\n        num_features = last_layer_layer.in_features # Get the number of input features for the sub-layer\n        last_layer[-1] = torch.nn.Linear(num_features, 1) # Replace the sub-layer with a linear layer with 1 output\n        # print(model) # Print the updated model architecture\n    elif model_=='EfficientNet_B5':\n        model = torchvision.models.efficientnet_b5(weights=torchvision.models.EfficientNet_B5_Weights)\n        last_layer = list(model.children())[-1] # Get the last layer of the model\n        last_layer_layer = list(last_layer.children())[-1] # Get the sub-layer of the last layer\n        num_features = last_layer_layer.in_features # Get the number of input features for the sub-layer\n        last_layer[-1] = torch.nn.Linear(num_features, 1) # Replace the sub-layer with a linear layer with 1 output\n        # print(model) # Print the updated model architecture\n    elif model_=='EfficientNet_B4':\n        model = torchvision.models.efficientnet_b4(weights=torchvision.models.EfficientNet_B4_Weights)\n        last_layer = list(model.children())[-1] # Get the last layer of the model\n        last_layer_layer = list(last_layer.children())[-1] # Get the sub-layer of the last layer\n        num_features = last_layer_layer.in_features # Get the number of input features for the sub-layer\n        last_layer[-1] = torch.nn.Linear(num_features, 1) # Replace the sub-layer with a linear layer with 1 output\n        # print(model) # Print the updated model architecture\n    elif model_=='EfficientNet_B3':\n        model = torchvision.models.efficientnet_b3(weights=torchvision.models.EfficientNet_B3_Weights)\n        last_layer = list(model.children())[-1] # Get the last layer of the model\n        last_layer_layer = list(last_layer.children())[-1] # Get the sub-layer of the last layer\n        num_features = last_layer_layer.in_features\n        last_layer[-1] = torch.nn.Linear(num_features, 1)\n    elif model_=='resnet18':\n        model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights)\n        num_ftrs = model.fc.in_features\n        model.fc = torch.nn.Linear(num_ftrs, 1)\n    elif model_=='resnet50':\n        model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights)\n        num_ftrs = model.fc.in_features\n        model.fc = torch.nn.Linear(num_ftrs, 1)\n    elif model_=='resnet101':\n        model = torchvision.models.resnet101(weights=torchvision.models.ResNet101_Weights)\n        num_ftrs = model.fc.in_features\n        model.fc = torch.nn.Linear(num_ftrs, 1)\n    elif model_=='resnet152':\n        model = torchvision.models.resnet152(weights=torchvision.models.ResNet152_Weights)\n        num_ftrs = model.fc.in_features\n        model.fc = torch.nn.Linear(num_ftrs, 1)\n    elif model_=='densenet121':\n        model = torchvision.models.densenet121(weights=torchvision.models.DenseNet121_Weights)\n        num_ftrs = model.classifier.in_features\n        model.classifier = torch.nn.Linear(num_ftrs, 1)\n    elif model_=='densenet169':\n        model = torchvision.models.densenet169(weights=torchvision.models.DenseNet169_Weights)\n        num_ftrs = model.classifier.in_features\n        model.classifier = torch.nn.Linear(num_ftrs, 1)\n    elif model_=='densenet201':\n        model = torchvision.models.densenet201(weights=torchvision.models.DenseNet201_Weights)\n        num_ftrs = model.classifier.in_features\n        model.classifier = torch.nn.Linear(num_ftrs, 1)\n    else:\n        raise ValueError('Model not found, please choose one of the following: EfficientNet_B7, EfficientNet_B6, EfficientNet_B5, EfficientNet_B4, EfficientNet_B3, resnet18, resnet50, resnet101, resnet152, densenet121, densenet169, densenet201')\n\n    if torch.cuda.device_count() > 1: # check if multiple GPUs are available\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n        model = torch.nn.DataParallel(model) # make parallel\n    model = model.to(device)\n    return model",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:27.047380Z",
     "iopub.execute_input": "2023-03-27T07:23:27.047897Z",
     "iopub.status.idle": "2023-03-27T07:23:27.067785Z",
     "shell.execute_reply.started": "2023-03-27T07:23:27.047855Z",
     "shell.execute_reply": "2023-03-27T07:23:27.066708Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def make_train_step(model, optimizer, loss_fn):\n    \"\"\"\n    Builds function that performs a step in the train loop\n    :param model: model\n    :param optimizer: optimizer\n    :param loss_fn: loss function\n    :return: train_step\n    \"\"\"\n    def train_step(x, y):\n        \"\"\"\n        Performs a train step\n        :param x: input\n        :param y: target\n        :return: loss\n        \"\"\"\n        yhat = model(x)  # Make prediction\n        model.train()  #enter train mode\n        loss = loss_fn(yhat, y)  #compute loss\n        loss.backward()  # backpropagation\n        optimizer.step()  #update weights\n        optimizer.zero_grad()  #zero gradients\n\n        return loss.item()\n\n    return train_step",
   "metadata": {
    "id": "mc4wjRDA2qaO",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:27.071072Z",
     "iopub.execute_input": "2023-03-27T07:23:27.071352Z",
     "iopub.status.idle": "2023-03-27T07:23:27.082140Z",
     "shell.execute_reply.started": "2023-03-27T07:23:27.071326Z",
     "shell.execute_reply": "2023-03-27T07:23:27.081108Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_train_step(model):\n    \"\"\"\n    Get train step\n    :param model: model\n    :return: train_step\n    \"\"\"\n\n    loss_fn = BCEWithLogitsLoss()  #binary cross entropy with sigmoid, so no need to use sigmoid in the model\n    optimizer = torch.optim.Adam(model.parameters())\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n    train_step = make_train_step(model, optimizer, loss_fn)\n\n    return train_step",
   "metadata": {
    "id": "O_FG_7ag2sW8",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:27.083690Z",
     "iopub.execute_input": "2023-03-27T07:23:27.084324Z",
     "iopub.status.idle": "2023-03-27T07:23:27.095910Z",
     "shell.execute_reply.started": "2023-03-27T07:23:27.084284Z",
     "shell.execute_reply": "2023-03-27T07:23:27.094867Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### TRAINING",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def save_weights(model, filename='best_weight.pt'):\n    \"\"\"\n    Save model weights\n    :param model: model\n    :return: None\n    \"\"\"\n    color = \"\\033[0;92m\" if filename == 'best_weight.pt' else \"\\033[0;93m\"\n    torch.save(model.state_dict(), ROOT_DIR + filename)\n    print(color+'Model weights saved at {}'.format(ROOT_DIR + filename))",
   "metadata": {
    "id": "UNFixOJyynlw",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:27.229380Z",
     "iopub.execute_input": "2023-03-27T07:23:27.229861Z",
     "iopub.status.idle": "2023-03-27T07:23:27.237414Z",
     "shell.execute_reply.started": "2023-03-27T07:23:27.229832Z",
     "shell.execute_reply": "2023-03-27T07:23:27.236332Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def train(model, batch=16, n_epochs=15, early_stopping_tolerance=3,\n          early_stopping_threshold=0.01):\n    \"\"\"\n    Train model\n    :param model: model\n    :param batch: batch size\n    :param n_epochs: number of epochs\n    :param early_stopping_tolerance: early stopping tolerance\n    :param early_stopping_threshold: early stopping threshold\n    :return:None\n    \"\"\"\n    train_step = get_train_step(model)\n    train_dataloader, val_dataloader = get_dataloaders(batch_size=batch)\n\n    loss_fn = BCEWithLogitsLoss()\n    losses = []\n    val_losses = []\n\n    epoch_train_losses = []\n    epoch_test_losses = []\n    early_stopping_counter = 0\n\n    for epoch in range(n_epochs):\n        epoch_loss = 0\n        for i, data in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):  #iterate ove batches\n            x_batch, y_batch = data\n            x_batch = x_batch.to(device)  #move to gpu\n            y_batch = y_batch.unsqueeze(1).float()  #convert target to same nn output shape\n            y_batch = y_batch.to(device)  #move to gpu\n\n            loss = train_step(x_batch, y_batch)\n            epoch_loss += loss / len(train_dataloader)\n            losses.append(loss)\n\n        epoch_train_losses.append(epoch_loss)\n        print('\\nEpoch : {}, train loss : {}'.format(epoch + 1, epoch_loss))\n\n        #validation doesnt requires gradient\n        with torch.no_grad():\n            cum_loss = 0\n            total = 0\n            correct = 0\n            for x_batch, y_batch in tqdm(val_dataloader, total=len(val_dataloader)):\n                x_batch = x_batch.to(device)\n                y_batch = y_batch.unsqueeze(1).float()  #convert target to same nn output shape\n                y_batch = y_batch.to(device)\n\n                #model to eval mode\n                model.eval()\n\n                yhat = model(x_batch)\n                val_loss = loss_fn(yhat, y_batch).item()\n                cum_loss += val_loss / len(val_dataloader)\n                val_losses.append(val_loss)\n\n                predicted = torch.round(torch.sigmoid(yhat))\n                total += y_batch.size(0)\n                correct += (predicted == y_batch).sum().item()\n\n            epoch_test_losses.append(cum_loss)\n            print('Epoch : {}, val loss : {}'.format(epoch + 1, cum_loss))\n            print('Accuracy of the model on the validation set: %.2f%%' % (100 * correct / total))\n\n            best_loss = min(epoch_test_losses)\n\n            #save best model\n            if cum_loss <= best_loss:\n                best_model_wts = model.state_dict()\n                save_weights(model)\n            else:\n                save_weights(model, filename='last_weight.pt')\n\n            if cum_loss > best_loss:\n                early_stopping_counter +=1 # add counter\n            else:\n                early_stopping_counter = 0 # reset counter\n\n            if (early_stopping_counter >= early_stopping_tolerance) or (best_loss <= early_stopping_threshold):\n                print(\"/nTerminating: early stopping\")\n                break  #terminate training\n\n    model.load_state_dict(torch.load(ROOT_DIR+\"best_weight.pt\"))\n    print(f'Finished Training, best loss : {best_loss}, best weights loaded in model')",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:27.918587Z",
     "iopub.execute_input": "2023-03-27T07:23:27.919030Z",
     "iopub.status.idle": "2023-03-27T07:23:27.935133Z",
     "shell.execute_reply.started": "2023-03-27T07:23:27.918987Z",
     "shell.execute_reply": "2023-03-27T07:23:27.933830Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### TESTING",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def check_val(model):\n    \"\"\"\n    Check validation loss and accuracy after training\n    :param model: model\n    :param val_dataloader: validation dataloader\n    :return: None\n    \"\"\"\n    # create confusion matrix\n    FP = 0\n    FN = 0\n    TP = 0\n    TN = 0\n\n    _, val_dataloader = get_dataloaders()\n    loss_fn = BCEWithLogitsLoss()\n    with torch.no_grad():\n        cum_loss = 0\n        total = 0\n        correct = 0\n        for x_batch, y_batch in tqdm(val_dataloader, total=len(val_dataloader)):\n            x_batch = x_batch.to(device)\n            y_batch = y_batch.unsqueeze(1).float()  #convert target to same nn output shape\n            y_batch = y_batch.to(device)\n\n            #model to eval mode\n            model.eval()\n\n            yhat = model(x_batch)\n            val_loss = loss_fn(yhat, y_batch).item()\n            cum_loss += val_loss / len(val_dataloader)\n\n            # confusion matrix\n            predicted = torch.round(torch.sigmoid(yhat))\n\n            for i in range(len(predicted)):\n                if predicted[i] == 1 and y_batch[i] == 0:\n                    FP += 1\n                elif predicted[i] == 0 and y_batch[i] == 1:\n                    FN += 1\n                elif predicted[i] == 1 and y_batch[i] == 1:\n                    TP += 1\n                elif predicted[i] == 0 and y_batch[i] == 0:\n                    TN += 1\n\n            total += y_batch.size(0)\n            correct += (predicted == y_batch).sum().item()\n\n        print('Accuracy of the model on the validation set: %.2f%%' % (100 * correct / total))\n        print('Loss of the model on the validation set: %.2f' % (cum_loss))\n        print('Confusion matrix: \\n', 'TP: ', TP, 'FP: ', FP, 'FN: ', FN, 'TN: ', TN)\n        print('Precision: ', TP/(TP+FP))\n        print('Recall: ', TP/(TP+FN))\n        print('F1 score: ', 2*TP/(2*TP+FP+FN))\n        print('Specificity: ', TN/(TN+FP))\n        print('Accuracy: ', (TP+TN)/(TP+TN+FP+FN))\n        print('MCC: ', (TP*TN-FP*FN)/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)))\n        print('FPR: ', FP/(FP+TN))\n        cm = np.array([[TP, FP], [FN, TN]])\n        plt.figure(figsize=(5,5))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n        plt.title('Confusion matrix')\n        plt.ylabel('Actual label')\n        plt.xlabel('Predicted label')\n        plt.show()\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "361ZTugaqQsD",
    "outputId": "db2f6ca3-9954-4174-acde-818127081ae0",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:28.867575Z",
     "iopub.execute_input": "2023-03-27T07:23:28.868572Z",
     "iopub.status.idle": "2023-03-27T07:23:28.883931Z",
     "shell.execute_reply.started": "2023-03-27T07:23:28.868530Z",
     "shell.execute_reply": "2023-03-27T07:23:28.882771Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def predict(model, filename_='classifier.csv'):\n    \"\"\"\n    Predict test data\n    :param model: model\n    :return: None\n    \"\"\"\n    _, _, test_transforms = get_transformers()\n    model.eval()\n    results = []\n\n    for filename in tqdm(os.listdir(TEST_DIR)):\n        img = Image.open(os.path.join(TEST_DIR, filename)).convert('RGB')\n        img = test_transforms(img)\n        img = img.unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            output = model(img)\n            predicted = torch.round(torch.sigmoid(output))\n            confidence = torch.sigmoid(output)\n            results.append((filename.split('.')[0], confidence.item()))\n\n    pred_classifier = pd.DataFrame(results, columns=['image_id', 'target'])\n    pred_classifier.to_csv(filename_, index=False)\n    display(pred_classifier.head())\n    print('Predictions saved at {}'.format(ROOT_DIR + filename_))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "Ee5k12L22wQ2",
    "outputId": "f9051364-3dd5-411f-f4a8-39a7cea64309",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:29.464609Z",
     "iopub.execute_input": "2023-03-27T07:23:29.465310Z",
     "iopub.status.idle": "2023-03-27T07:23:29.473809Z",
     "shell.execute_reply.started": "2023-03-27T07:23:29.465266Z",
     "shell.execute_reply": "2023-03-27T07:23:29.472527Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## RUN",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### DOWNLOAD DATA",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "download_data(image_size=256)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11NHcdE8Z3u7",
    "outputId": "43a797e4-7e5c-472e-c1d1-97d8b07efec9",
    "execution": {
     "iopub.status.busy": "2023-03-27T07:23:31.340061Z",
     "iopub.execute_input": "2023-03-27T07:23:31.340433Z",
     "iopub.status.idle": "2023-03-27T07:24:02.726121Z",
     "shell.execute_reply.started": "2023-03-27T07:23:31.340399Z",
     "shell.execute_reply": "2023-03-27T07:24:02.724818Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### PREPROCESS DATA",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data_preparation()",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:24:02.728679Z",
     "iopub.execute_input": "2023-03-27T07:24:02.729388Z",
     "iopub.status.idle": "2023-03-27T07:24:36.499803Z",
     "shell.execute_reply.started": "2023-03-27T07:24:02.729332Z",
     "shell.execute_reply": "2023-03-27T07:24:36.498596Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train: 12500 - Val: 2500\nCopy files...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 67914/67914 [00:33<00:00, 2028.93it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Train No Finding: 8836\nTrain Finding: 3664\nVal No Finding: 1770\nVal Finding: 730\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### MODEL",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model = get_model(\"EfficientNet_B4\")",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:24:36.501629Z",
     "iopub.execute_input": "2023-03-27T07:24:36.502028Z",
     "iopub.status.idle": "2023-03-27T07:24:41.956640Z",
     "shell.execute_reply.started": "2023-03-27T07:24:36.501990Z",
     "shell.execute_reply": "2023-03-27T07:24:41.955595Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-7eb33cd5.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-7eb33cd5.pth\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0.00/74.5M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27484913ade0430a9dc7978e1a1f8d3d"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Let's use 2 GPUs!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### TRAIN",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train(model, batch=16, n_epochs=5, early_stopping_tolerance=3, early_stopping_threshold=0.03)",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:24:41.959079Z",
     "iopub.execute_input": "2023-03-27T07:24:41.959465Z",
     "iopub.status.idle": "2023-03-27T07:44:51.622727Z",
     "shell.execute_reply.started": "2023-03-27T07:24:41.959428Z",
     "shell.execute_reply": "2023-03-27T07:44:51.621334Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train: 12500 - Val: 2500\nIdx to class: {'finding': 0, 'no_finding': 1}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 782/782 [03:51<00:00,  3.38it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\nEpoch : 1, train loss : 0.31021870846581445\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 157/157 [00:20<00:00,  7.54it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch : 1, val loss : 0.20689308403688622\nAccuracy of the model on the validation set: 91.56%\n\u001B[0;92mModel weights saved at /kaggle/best_weight.pt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 782/782 [03:38<00:00,  3.58it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\nEpoch : 2, train loss : 0.21166268607977853\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 157/157 [00:21<00:00,  7.33it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch : 2, val loss : 0.17347936882714554\nAccuracy of the model on the validation set: 92.96%\n\u001B[0;92mModel weights saved at /kaggle/best_weight.pt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 782/782 [03:37<00:00,  3.59it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\nEpoch : 3, train loss : 0.17748933390873814\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 157/157 [00:21<00:00,  7.47it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch : 3, val loss : 0.1377970537624565\nAccuracy of the model on the validation set: 94.80%\n\u001B[0;92mModel weights saved at /kaggle/best_weight.pt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 782/782 [03:37<00:00,  3.60it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\nEpoch : 4, train loss : 0.15077971814729083\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 157/157 [00:21<00:00,  7.44it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch : 4, val loss : 0.16116000743654021\nAccuracy of the model on the validation set: 93.96%\n\u001B[0;93mModel weights saved at /kaggle/last_weight.pt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 782/782 [03:36<00:00,  3.61it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\nEpoch : 5, train loss : 0.14062116886078205\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 157/157 [00:21<00:00,  7.36it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch : 5, val loss : 0.13884319970235937\nAccuracy of the model on the validation set: 94.88%\n\u001B[0;93mModel weights saved at /kaggle/last_weight.pt\nFinished Training, best loss : 0.1377970537624565, best weights loaded in model\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### CHECK VAL",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "check_val(model)",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-27T07:44:55.555311Z",
     "iopub.execute_input": "2023-03-27T07:44:55.555690Z",
     "iopub.status.idle": "2023-03-27T07:45:16.990818Z",
     "shell.execute_reply.started": "2023-03-27T07:44:55.555655Z",
     "shell.execute_reply": "2023-03-27T07:45:16.989092Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train: 12500 - Val: 2500\nIdx to class: {'finding': 0, 'no_finding': 1}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 157/157 [00:21<00:00,  7.46it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Accuracy of the model on the validation set: 94.80%\nLoss of the model on the validation set: 0.14\nConfusion matrix: \n TP:  1742 FP:  102 FN:  28 TN:  628\nPrecision:  0.9446854663774403\nRecall:  0.984180790960452\nF1 score:  0.9640287769784173\nSpecificity:  0.8602739726027397\nAccuracy:  0.948\nMCC:  0.8727544564581439\nFPR:  0.13972602739726028\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 500x500 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### PREDICT",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "predict(model)",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-03-26T13:10:30.701508Z",
     "iopub.execute_input": "2023-03-26T13:10:30.703414Z",
     "iopub.status.idle": "2023-03-26T13:13:31.698158Z",
     "shell.execute_reply.started": "2023-03-26T13:10:30.703371Z",
     "shell.execute_reply": "2023-03-26T13:13:31.696835Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 3000/3000 [03:00<00:00, 16.58it/s]\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                           image_id    target\n0  c9fbdd6dbbd3e5246b0184824f681e0f  0.860541\n1  b12232ebf575db0226671621146f76e1  0.999992\n2  e713fe6ed28f23aed83f54c56ff30189  0.999979\n3  754b90e60a89d03d18dbf9b30cce6eec  0.999465\n4  d7494cea30e4fa1e4b4ebd0ecc6e03e5  0.692806",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c9fbdd6dbbd3e5246b0184824f681e0f</td>\n      <td>0.860541</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b12232ebf575db0226671621146f76e1</td>\n      <td>0.999992</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e713fe6ed28f23aed83f54c56ff30189</td>\n      <td>0.999979</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>754b90e60a89d03d18dbf9b30cce6eec</td>\n      <td>0.999465</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d7494cea30e4fa1e4b4ebd0ecc6e03e5</td>\n      <td>0.692806</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Predictions saved at /kaggle/classifier.csv\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## FINE TUNING",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### FINE TUNING LOOP THROUGH MODELS",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "check_val(model)\npredict(model, filename_=f\"working/EfficientNet_B6.csv\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "models = [\"EfficientNet_B5\", \"EfficientNet_B4\", \"EfficientNet_B3\", \"resnet18\", \"resnet50\", \"resnet101\", \"resnet152\", \"densenet121\", \"densenet169\", \"densenet201\"]\nfor model_ in models:\n    print(f\"Training {model_}\")\n    model = get_model(model_)\n    train(model, batch=16, n_epochs=10, early_stopping_tolerance=3, early_stopping_threshold=0.01)\n    check_val(model)\n    predict(model, filename_=f\"working/{model_}.csv\")\n    print(f\"Finished training {model_}\")\n    print(\"\")",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### FINE TUNING LOOP THROUGH BATCH SIZE",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "for batch in [8, 16, 32, 64]:\n    print(f\"Training with batch size {batch}\")\n    model = get_model()\n    train(model, batch=batch, n_epochs=10, early_stopping_tolerance=3, early_stopping_threshold=0.01)\n    check_val(model)\n    predict(model, filename=f\"batch_{batch}.csv\")\n    print(f\"Finished training with batch size {batch}\")\n    print(\"\")",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### FINE TUNING LOOP THROUGH EPOCHS",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "for n_epochs in [5, 10, 15, 20]:\n    print(f\"Training with {n_epochs} epochs\")\n    model = get_model()\n    train(model, batch=16, n_epochs=n_epochs, early_stopping_tolerance=3, early_stopping_threshold=0.01)\n    check_val(model)\n    predict(model, filename=f\"epochs_{n_epochs}.csv\")\n    print(f\"Finished training with {n_epochs} epochs\")\n    print(\"\")",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  }
 ]
}
